# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DTHx0cwLnZlwUIiN-DHfXAh53qz0OGyp
"""

!pip install tensorflow numpy pandas matplotlib scikit-learn

import pandas as pd

# Load the dataset
df = pd.read_csv('/HomeC.csv', sep=',', infer_datetime_format=True, low_memory=False, index_col='time', encoding='utf-8')

# Check the first few rows to understand the structure
df.head()

# Convert index to datetime (handling any potential issues with the format)
df.index = pd.to_datetime(df.index, unit='s', errors='coerce')

# Drop any rows where the index is NaT (invalid datetime values)
df = df[df.index.notna()]

# Sort the index after conversion
df.sort_index(inplace=True)

# Drop any remaining rows with NaN values in the columns, but avoid SettingWithCopyWarning
df = df.dropna(inplace=False)

# Check for non-numeric columns
# Print the column types to identify any non-numeric columns
print(df.dtypes)

# Convert any columns with non-numeric values to NaN and then drop them
# This will handle the 'clear-night' type of string values in the data
df = df.apply(pd.to_numeric, errors='coerce')

# Now proceed with resampling the data to 1-hour frequency (if not already done)
df = df.resample('1h').mean()

# Apply exponential moving average to smooth the data (smoothing with alpha=0.2)
df = df.ewm(alpha=0.2).mean()

# Drop any rows with NaN values after smoothing
df.dropna(inplace=True)

# Outlier removal based on 3 standard deviations from the mean (using 'House overall [kW]')
factor = 3
upper_lim = df['House overall [kW]'].mean() + df['House overall [kW]'].std() * factor
lower_lim = df['House overall [kW]'].mean() - df['House overall [kW]'].std() * factor

# Keep only values within the defined upper and lower limits
df = df[(df['House overall [kW]'] < upper_lim) & (df['House overall [kW]'] > lower_lim)]

# Display the first few rows to ensure everything is correct
df.head()

import numpy as np
import pandas as pd
from collections import deque

# Define sequence length (how many hours we will use to predict the next values)
SEQ_LEN = 100

# Function to preprocess the data and create sequences
def preprocess_df(df, seq_len=SEQ_LEN):
    sequential_data = []
    prev_days = deque(maxlen=seq_len)

    for i in df.values:
        prev_days.append([n for n in i[:-1]])  # Keep all columns except the target
        if len(prev_days) == seq_len:
            sequential_data.append([np.array(prev_days), i[-1]])  # Last value is the target (next value to predict)

    # Convert to numpy arrays
    X = []
    y = []

    for seq, target in sequential_data:
        X.append(seq)  # Features
        y.append(target)  # Target value

    # Convert lists to numpy arrays
    X = np.array(X)
    y = np.array(y)

    print("Shape of X:", X.shape)
    print("Shape of y:", y.shape)

    return X, y

# Check how the data is loaded (replace this with your data loading method)
# Assuming you load your data from a CSV
df = pd.read_csv('/HomeC.csv')

# Verify the shape and content of the dataframe before proceeding
print("Shape of df:", df.shape)
print("Columns in df:", df.columns)

# Ensure the dataframe is not empty before proceeding
if df.shape[0] > 0:
    # Select the columns you want to use for prediction (including additional features)
    columns_to_use = ['House overall [kW]', 'temperature', 'humidity', 'windSpeed']  # Add or remove features as needed

    # Prepare the data
    X, y = preprocess_df(df[columns_to_use])

    # Ensure that X and y are not empty before splitting
    if len(X) > 0 and len(y) > 0:
        # Split the data into training and testing sets (80% train, 20% test)
        train_size = int(0.8 * len(X))
        X_train, X_test = X[:train_size], X[train_size:]
        y_train, y_test = y[:train_size], y[train_size:]

        # Reshape the data for LSTM (samples, time steps, features)
        # Since X_train and X_test already have the shape (samples, time steps, features), no additional reshaping is needed
        print("X_train shape after preprocessing:", X_train.shape)
        print("X_test shape after preprocessing:", X_test.shape)
    else:
        print("Error: X and/or y are empty. Please check the preprocessing step.")
else:
    print("Error: DataFrame is empty. Check the data source.")

from tensorflow.keras.models import Model
from tensorflow.keras.layers import LSTM, Dense, Dropout, Attention, LayerNormalization, Add
from tensorflow.keras import Input
import tensorflow as tf

# Function to create LSTM + Attention model for multiple predictions
def create_lstm_attention_model(input_shape, y_train):
    inputs = Input(shape=input_shape)

    # First LSTM layer
    x = LSTM(64, activation='relu', return_sequences=True)(inputs)
    x = Dropout(0.2)(x)  # Dropout for regularization

    # Attention Layer (using query and value from the same tensor x)
    attention_output = Attention()([x, x])  # Query and value are the same here (self-attention)
    attention_output = LayerNormalization()(attention_output)  # Normalize the attention output
    x = Add()([attention_output, x])  # Add residual connection (skip connection)

    # Second LSTM layer
    x = LSTM(64, activation='relu', return_sequences=False)(x)
    x = Dropout(0.2)(x)

    # Output layer for multiple predictions
    output = Dense(y_train.shape[1])(x)  # This works since y_train is now 2D with shape (num_samples, 1)

    # Create the model
    model = tf.keras.models.Model(inputs=inputs, outputs=output)

    # Compile the model
    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])

    return model

# Reshape y_train to be 2D (if it's a single prediction per sample)
y_train = y_train.reshape(-1, 1)  # Now y_train has the shape (403049, 1)

# Check shapes of X_train and y_train
print(X_train.shape)  # Should be (403049, 100, 3)
print(y_train.shape)  # Should be (403049, 1)

# Create the model
model = create_lstm_attention_model((X_train.shape[1], X_train.shape[2]), y_train)
model.summary()  # Display the model architecture

# Train the model with fewer epochs and smaller batch size
history = model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))

# Print training history (loss over epochs)
for epoch in range(5):  # Loop through each epoch
    print(f"Epoch {epoch+1}/{5}")
    print(f"  Training Loss: {history.history['loss'][epoch]}")
    print(f"  Validation Loss: {history.history['val_loss'][epoch]}")

import numpy as np
from sklearn.metrics import mean_squared_error, mean_absolute_error

# Make predictions
y_pred = model.predict(X_test)

# Fix shapes
y_test = y_test.reshape(-1)
y_pred = y_pred.reshape(-1)

# Remove NaNs safely
valid_idx = ~np.isnan(y_test) & ~np.isnan(y_pred)

y_test_clean = y_test[valid_idx]
y_pred_clean = y_pred[valid_idx]

print("Valid test samples:", len(y_test_clean))

# Compute metrics
rmse = np.sqrt(mean_squared_error(y_test_clean, y_pred_clean))
mae = mean_absolute_error(y_test_clean, y_pred_clean)

def smape(y_true, y_pred):
    y_true = np.nan_to_num(y_true)
    y_pred = np.nan_to_num(y_pred)
    denom = np.abs(y_true) + np.abs(y_pred)
    diff = np.abs(y_true - y_pred) / denom
    diff[denom == 0] = 0
    return 200 * np.mean(diff)

smape_value = smape(y_test_clean, y_pred_clean)

print(f"Test RMSE: {rmse}")
print(f"Test MAE: {mae}")
print(f"Test sMAPE: {smape_value}%")

# Example prediction
prediction = model.predict(X_test[0:1])
print("Prediction:", prediction)

# Example: Predicting the next value
new_data = X_test[0:1]  # You can change this to any data point you'd like to predict
prediction = model.predict(new_data)
print(f'Prediction: {prediction}')